# Microservice
## 1. Document the microservice architecture and components/tools/dependencies

In a Java Spring microservice architecture, the following components, tools, and dependencies are typically documented:

- **Spring Boot**: Used for building and running the microservices with minimal configuration.
- **Spring Cloud**: Provides tools for managing configuration, service discovery, and circuit breakers.
- **Spring Cloud Netflix**: Offers Netflix OSS-based components such as Eureka for service discovery, Ribbon for client-side load balancing, and Hystrix for circuit breaking.
- **Spring Data**: Simplifies data access and integration with various databases.
- **Spring Security**: Manages authentication and authorization.
- **Spring Actuator**: Provides production-ready features to help you monitor and manage your application.
- **Zuul or Spring Cloud Gateway**: Used as an API Gateway for routing requests to appropriate microservices.
- **Config Server**: Manages external configurations for applications across all environments.
- **Message Brokers (e.g., Kafka, RabbitMQ)**: Facilitate asynchronous communication between microservices.
- **Docker**: Containerizes the microservices for consistent deployment across different environments.
- **Kubernetes**: Orchestrates containerized applications for scaling and management.
- **Jenkins**: Automates the build and deployment pipeline.
- **Prometheus and Grafana**: Used for monitoring and observability of microservices.

Dependencies typically found in the `pom.xml` or `build.gradle` file include:

- `spring-boot-starter-web`
- `spring-boot-starter-data-jpa`
- `spring-boot-starter-security`
- `spring-boot-starter-actuator`
- `spring-cloud-starter-netflix-eureka-client`
- `spring-cloud-starter-netflix-ribbon`
- `spring-cloud-starter-netflix-hystrix`
- `spring-cloud-starter-config`
- `spring-cloud-starter-zuul`
- `spring-cloud-starter-gateway`
- `spring-kafka` or `spring-rabbit`

Documenting these components helps ensure a clear understanding of the architecture and how different parts of the system interact.

## 2. What are Resilience patterns? What is a circuit breaker?

Resilience patterns are design strategies used to build systems that can withstand and gracefully recover from failures. These patterns ensure that the system remains functional and responsive even when some components fail or experience high load. Common resilience patterns include:

1. **Retry**: Automatically retrying failed operations a certain number of times before giving up.
2. **Timeout**: Setting a maximum time limit for operations to complete to avoid indefinite waits.
3. **Fallback**: Providing a backup or alternative method when the primary operation fails.
4. **Bulkhead**: Isolating different parts of a system to prevent a failure in one part from cascading to others.
5. **Rate Limiting**: Controlling the number of requests sent to a service to prevent overloading.

A **circuit breaker** is a specific resilience pattern used to detect and handle failures gracefully. It works similarly to an electrical circuit breaker. When the number of failures for a particular operation exceeds a threshold, the circuit breaker trips and prevents further attempts to perform the operation for a specified period. This helps to avoid overwhelming a failing service and allows time for recovery. The circuit breaker typically has three states:

1. **Closed**: The system operates normally, and requests are allowed through.
2. **Open**: Requests are blocked immediately, and an error response is returned without attempting the operation.
3. **Half-Open**: A limited number of requests are allowed through to test if the service has recovered. If successful, the circuit breaker transitions to the closed state; otherwise, it remains open.

Using a circuit breaker helps to improve the overall resilience and stability of a microservices architecture by preventing cascading failures and reducing the impact of service disruptions.

## 3. Read this article, then list the important questions, then write your answers: ([https://www.interviewbit.com/microservices-interview-questions/#main-features-of-microservices](https://www.interviewbit.com/microservices-interview-questions/#main-features-of-microservices))

- **Decoupling**: Within a system, services are largely decoupled. The application as a whole can therefore be easily constructed, altered, and scalable
- **Componentization**: Microservices are viewed as independent components that can easily be exchanged or upgraded
- **Business Capabilities**: Microservices are relatively simple and only focus on one service
- **Team autonomy**: Each developer works independently of each other, allowing for a faster project timeline
- **Continuous Delivery**: Enables frequent software releases through systematic automation of software development, testing, and approval
- **Responsibility:** Microservices are not focused on applications as projects. Rather, they see applications as products they are responsible for
- **Decentralized Governance:** Choosing the right tool according to the job is the goal. Developers can choose the best tools to solve their problems
- **Agility:** Microservices facilitate agile development. It is possible to create new features quickly and discard them again at any time

### **What do you mean by Cohesion and Coupling?**

**Coupling**: It is defined as a relationship between software modules A and B, and how much one module depends or interacts with another one. Couplings fall into three major categories. Modules can be highly coupled (highly dependent), loosely coupled, and uncoupled from each other. The best kind of coupling is loose coupling, which is achieved through interfaces.

**Cohesion**: It is defined as a relationship between two or more parts/elements of a module that serves the same purpose. Generally, a module with high cohesion can perform a specific function efficiently without needing communication with any other modules. High cohesion enhances the functionality of the module.

### **What do you mean by Bounded Context?**

A Bounded Context is a central pattern in DDD (Domain-Driven Design), which deals with collaboration across large models and teams. DDD breaks large models down into multiple contexts to make them more manageable. Additionally, it explains their relationship explicitly. The concept promotes an object-oriented approach to developing services bound to a data model and is also responsible for ensuring the integrity and mutability of said data model.

### 3.1. Microservices Interview Questions

## 4. How to do load balancing in microservices? Write a long summary by yourself:

Load balancing in microservices is a critical practice that ensures the even distribution of incoming network traffic across multiple servers or instances. This helps in enhancing the reliability, availability, and performance of applications. Here's a comprehensive summary of how load balancing can be achieved in microservices architectures:

### 1. **Types of Load Balancing**

- **Client-Side Load Balancing**: In this approach, the client is responsible for distributing the load. The client maintains a list of service instances and selects one based on a predefined algorithm. Tools like Netflix Ribbon can be used to implement client-side load balancing.
- **Server-Side Load Balancing**: Here, a load balancer sits between the client and the server instances. It distributes incoming requests to different servers based on various algorithms. Examples include NGINX, HAProxy, and AWS Elastic Load Balancer (ELB).
- **DNS Load Balancing**: This method involves using DNS to distribute traffic. Multiple IP addresses are associated with a single domain name, and the DNS server rotates the addresses. However, this method has limitations in terms of dynamic scaling and health checks.

### 2. **Load Balancing Algorithms**

- **Round Robin**: Requests are distributed evenly across all available instances in a cyclical manner.
- **Least Connections**: The request is sent to the server with the least number of active connections.
- **IP Hash**: The client’s IP address is used to determine which server will handle the request, ensuring that the same client is consistently directed to the same server.
- **Weighted Round Robin/Least Connections**: Servers are assigned weights based on their capacity, and requests are distributed proportionally.

### 3. **Service Discovery Integration**

- **Service Registry**: A central registry (like Eureka, Consul, or etcd) keeps track of all available service instances and their health status. Load balancers query this registry to get the list of active instances.
- **Dynamic Scaling**: As new instances are added or removed, the service registry updates automatically, and the load balancer adjusts the distribution of requests accordingly.

### 4. **Health Checks**

- **Active Health Checks**: The load balancer periodically pings service instances to check their health. Unhealthy instances are removed from the rotation.
- **Passive Health Checks**: The load balancer monitors the responses from service instances. If an instance starts failing, it is marked as unhealthy.

### 5. **Global Load Balancing**

- **Geographical Load Balancing**: Directs traffic based on the geographical location of the client and server. This reduces latency and improves user experience.
- **Multi-Cloud Load Balancing**: Distributes traffic across different cloud providers to enhance redundancy and avoid vendor lock-in.

### 6. **Security Considerations**

- **SSL Termination**: The load balancer handles SSL/TLS encryption and decryption, offloading this heavy task from the service instances.
- **DDoS Protection**: Load balancers can help mitigate Distributed Denial of Service (DDoS) attacks by distributing malicious traffic and ensuring that no single instance is overwhelmed.

### 7. **Monitoring and Observability**

- **Metrics Collection**: Load balancers provide metrics such as request rates, error rates, and latency, which are critical for monitoring the health and performance of the system.
- **Logging**: Detailed logs of the traffic can help in identifying issues and understanding the system's behavior under different conditions.

### 8. **Implementing Load Balancing in Microservices**

- **API Gateway**: Acts as a single entry point for all client requests and handles routing, load balancing, and security.
- **Service Mesh**: A dedicated infrastructure layer (like Istio or Linkerd) handles service-to-service communication, including load balancing, without requiring changes to the application code.

In summary, load balancing in microservices involves a combination of client-side and server-side strategies, integration with service discovery mechanisms, regular health checks, and monitoring to ensure efficient and reliable distribution of traffic. The choice of load balancing method and tools depends on the specific requirements of the application, such as scalability, fault tolerance, and performance.

### 4.1. Load Balancer System Design Interview Question

**Question: Design a load balancer for a high-traffic web application. Describe the architecture, components, and any considerations for scalability, fault tolerance, and security.**

**Answer:**

When designing a load balancer for a high-traffic web application, several factors need to be considered to ensure scalability, fault tolerance, and security. Here is a comprehensive design:

### 1. **Architecture Overview**

- **Client Requests**: Users send requests to the web application.
- **Load Balancer**: The load balancer receives client requests and distributes them to multiple backend servers.
- **Backend Servers**: These servers handle the actual processing of the requests and return responses to the load balancer.
- **Service Discovery**: Keeps track of available backend servers and their health status.
- **Database**: Stores and retrieves application data.

### 2. **Components**

- **Load Balancer**: Can be implemented using software solutions like NGINX, HAProxy, or cloud-based solutions like AWS Elastic Load Balancer (ELB).
- **Service Discovery**: Tools like Eureka, Consul, or etcd to keep track of available and healthy backend servers.
- **Backend Servers**: Multiple instances of the web application to handle incoming requests.
- **Health Checks**: Mechanisms to monitor the health of backend servers.
- **Monitoring and Logging**: Tools like Prometheus and Grafana for monitoring, and ELK stack (Elasticsearch, Logstash, Kibana) for logging.

### 3. **Load Balancing Algorithms**

- **Round Robin**: Distributes requests evenly across all servers.
- **Least Connections**: Sends requests to the server with the fewest active connections.
- **IP Hash**: Ensures requests from the same client are directed to the same server.
- **Weighted Round Robin/Least Connections**: Distributes requests based on server capacity.

### 4. **Scalability Considerations**

- **Horizontal Scaling**: Add more backend servers to handle increased traffic.
- **Auto-Scaling**: Automatically add or remove servers based on traffic load using cloud provider features.
- **Service Discovery Integration**: Automatically update the load balancer with new or removed servers.

### 5. **Fault Tolerance**

- **Health Checks**: Regularly check the health of backend servers and remove unhealthy ones from the rotation.
- **Redundancy**: Deploy multiple instances of the load balancer to avoid a single point of failure.
- **Failover Mechanisms**: Automatically switch to backup servers or regions in case of primary server failure.

### 6. **Security**

- **SSL/TLS Termination**: Handle SSL/TLS encryption at the load balancer to offload this task from backend servers.
- **DDoS Protection**: Use rate limiting and IP blacklisting to mitigate DDoS attacks.
- **Firewall and Security Groups**: Implement firewall rules to restrict access to backend servers.

### 7. **Monitoring and Observability**

- **Metrics Collection**: Monitor request rates, error rates, and response times.
- **Logging**: Capture detailed logs for troubleshooting and analysis.
- **Alerts**: Set up alerts for unusual patterns or performance degradation.

### 8. **Implementation Steps**

1. **Set Up Load Balancer**: Deploy the load balancer and configure it to distribute traffic using the chosen algorithm.
2. **Configure Service Discovery**: Set up a service registry to track available backend servers.
3. **Deploy Backend Servers**: Launch multiple instances of the web application.
4. **Implement Health Checks**: Configure health checks to monitor server health.
5. **Enable Monitoring and Logging**: Set up monitoring and logging tools.
6. **Configure Security**: Implement SSL/TLS termination and security rules.

By following this design, the load balancer can efficiently distribute traffic, handle high loads, ensure fault tolerance, and maintain security for a high-traffic web application.

### 4.2. Load Balancing Interview Questions

## 5. How to do service discovery?

Service discovery is a mechanism used in microservices architectures to automatically detect and keep track of the available instances of services. It ensures that services can dynamically find and communicate with each other without hardcoding the network locations of service instances. Here is how service discovery can be implemented:

### 1. **Service Registry**

A service registry is a central database that maintains a list of service instances and their network locations. Examples include Eureka, Consul, and etcd.

- **Registration**: When a service instance starts, it registers itself with the service registry.
- **Deregistration**: When a service instance shuts down, it deregisters itself from the registry.
- **Heartbeat**: Service instances periodically send heartbeat signals to the registry to indicate they are still alive.

### 2. **Service Discovery Methods**

- **Client-Side Service Discovery**: The client queries the service registry to get the network location of a service instance and then makes a request directly to that instance. Libraries like Netflix Ribbon can be used for client-side service discovery.

  **Pros**:

    - Reduces load on the discovery server.
    - Clients have control over load balancing.

  **Cons**:

    - Requires clients to be aware of the discovery mechanism.
    - More complex client logic.
- **Server-Side Service Discovery**: The client makes a request to a load balancer, which queries the service registry and forwards the request to an available service instance. Examples of tools include NGINX, HAProxy, and AWS Elastic Load Balancer (ELB).

  **Pros**:

    - Simplifies client logic.
    - Centralized management of service discovery.

  **Cons**:

    - Load balancer becomes a potential bottleneck.
    - Additional infrastructure needed.

### 3. **Service Registry Tools**

- **Eureka**: A service registry and discovery tool developed by Netflix.
- **Consul**: Provides service discovery, configuration, and segmentation functionalities.
- **etcd**: A distributed key-value store that can be used for service discovery.

### 4. **Service Instance Communication**

- **HTTP/REST**: Services communicate over HTTP using RESTful APIs.
- **gRPC**: A high-performance RPC framework that uses HTTP/2.
- **Message Brokers**: Services communicate asynchronously through message brokers like Kafka or RabbitMQ.

### 5. **Health Checks**

- **Active Health Checks**: The service registry periodically checks the health of registered services.
- **Passive Health Checks**: The registry relies on requests and responses to determine the health of services.

### 6. **Service Discovery Configuration**

- **Configuration Management**: Externalize service configuration using tools like Spring Cloud Config.
- **Dynamic Scaling**: Automatically update the service registry as new instances are added or removed.

### 7. **Security**

- **Authentication and Authorization**: Secure the service registry with authentication and authorization mechanisms.
- **SSL/TLS**: Encrypt communication between services and the service registry.

By implementing service discovery, microservices can dynamically locate each other, enabling scalability, flexibility, and resilience in a distributed system.

### 6.1. Kafka Interview Questions

- Kafka Interview Questions for Freshers

  **1. What does it mean if a replica is not an In-Sync Replica for a long time?**

  > If a replica is not an In-Sync Replica (ISR) for a long time, it means the replica is not keeping up with the leader's log updates and may be lagging significantly, potentially leading to data inconsistency and reduced fault tolerance.
  >

  **2. What are the traditional methods of message transfer? How is Kafka better from them?**

  > **Traditional Methods of Message Transfer:**
  >

  > **Message Queues (e.g., RabbitMQ, ActiveMQ)**:
  >
  > - **Push Model**: Messages are pushed to consumers.
  > - **Broker-Centric**: The broker manages the state and delivery of messages.
  > - **Limited Scalability**: Often limited in terms of horizontal scaling.
  > - **Guaranteed Delivery**: Ensures messages are delivered at least once.

  > **Enterprise Service Bus (ESB)**:
  >
  > - **Centralized Architecture**: Acts as a central backbone to integrate various services.
  > - **Complexity**: Can become complex and hard to manage as the number of services grows.
  > - **Single Point of Failure**: Can be a bottleneck and single point of failure.

  > **Database Polling**:
  >
  > - **Polling Model**: Consumers poll the database for new records.
  > - **High Latency**: Introduces latency due to the polling interval.
  > - **Database Load**: Puts additional load on the database system.

  > **How Kafka is Better:**
  >

  > **Scalability**:
  >
  > - **Distributed Architecture**: Kafka is designed to handle large-scale data streams and can be horizontally scaled by adding more brokers.
  > - **Partitioning**: Data is partitioned across multiple brokers, allowing parallel processing.

  > **Performance**:
  >
  > - **High Throughput**: Capable of handling millions of messages per second with low latency.
  > - **Efficient Storage**: Uses a log-based storage mechanism for efficient read and write operations.

  > **Durability and Fault Tolerance**:
  >
  > - **Replication**: Messages are replicated across multiple brokers to ensure durability.
  > - **In-Sync Replicas (ISR)**: Ensures that data is synchronized across replicas for fault tolerance.

  > **Decoupling of Producers and Consumers**:
  >
  > - **Publish-Subscribe Model**: Producers publish messages to topics, and consumers subscribe to topics independently.
  > - **Decoupled Scaling**: Producers and consumers can scale independently.

  > **Retention Policies**:
  >
  > - **Configurable Retention**: Messages can be retained for a specified duration, allowing consumers to reprocess data if needed.
  > - **Log Compaction**: Option to retain only the latest value for a key, reducing storage requirements.

  > **Real-Time Processing**:
  >
  > - **Stream Processing**: Supports real-time stream processing through Kafka Streams and integrations with frameworks like Apache Flink and Apache Spark.

  > **Integration**:
  >
  > - **Ecosystem**: Rich ecosystem with connectors for various data sources and sinks, making it easier to integrate with other systems.

  **3. What are the major components of Kafka?**

  The major components of Kafka are:

    1. **Broker**: Manages the storage and retrieval of messages.
    2. **Topic**: A category or feed to which records are sent.
    3. **Producer**: Publishes messages to topics.
    4. **Consumer**: Subscribes to topics to read messages.
    5. **ZooKeeper**: Coordinates and manages Kafka brokers.
    6. **Partition**: A division of a topic for scalability and parallelism.
    7. **Replica**: A copy of a partition for fault tolerance.

  **4. Explain the four core API architecture that Kafka uses.**

  Kafka uses four core APIs:

    1. **Producer API**: Allows applications to publish (write) messages to Kafka topics.
    2. **Consumer API**: Allows applications to subscribe to (read) messages from Kafka topics.
    3. **Streams API**: Enables applications to process and transform data in Kafka using stream processing.
    4. **Connector API**: Provides reusable producers and consumers that connect Kafka topics to existing applications or data systems.

  **5. What do you mean by a Partition in Kafka?**

  A partition in Kafka is a division of a topic's data, allowing parallel processing and scalability by distributing the data across multiple brokers.

- Kafka Interview Questions for Experienced
- MCQ on Kafka